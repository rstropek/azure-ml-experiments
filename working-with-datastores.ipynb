{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\r\n",
        "\r\n",
        "This notebook is used to demonstrate how to work with AzureML Jupyter Notebooks.\r\n",
        "\r\n",
        "The sample data was taken from *kaggle*'s [*What's Cooking*](https://www.kaggle.com/c/whats-cooking/overview) competition."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working With Datastores\r\n",
        "\r\n",
        "[Datastores](https://docs.microsoft.com/en-us/azure/machine-learning/concept-data#connect-to-storage-with-datastores) are references to \r\n",
        "Azure storage services. You create a datastore and reference it from within your code. Therefore, you do not need to include connection\r\n",
        "information, secret keys, etc. in your code.\r\n",
        "\r\n",
        "Use the [`Datastore` class in `azureml.core`](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore.datastore?view=azure-ml-py) \r\n",
        "to work with AzureML Datastores."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authentication and Workspace Reference\r\n",
        "\r\n",
        "To get the workspace, use the `from_config()` method. In simple scenarios it should just work. Azure\r\n",
        "will care for the necessary configuration file. However, if you have multiple AAD tenants, you need\r\n",
        "to explicitly authentication (see [`azureml.core.authentication` package](azureml.core.authentication) for details)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\r\n",
        "from azureml.core.authentication import InteractiveLoginAuthentication\r\n",
        "\r\n",
        "# Change the following variable to your AAD tenant ID\r\n",
        "AAD_TENANT = '022e4faf-c745-475a-be06-06b1e1c9e39d'\r\n",
        "login = InteractiveLoginAuthentication(tenant_id = AAD_TENANT)\r\n",
        "\r\n",
        "ws = Workspace.from_config(auth = login)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615747303082
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iterating Over Datastores\r\n",
        "\r\n",
        "You can get a list of datastores from your workspace using the `datastores` method or your workspace."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for datastore in ws.datastores:\r\n",
        "    print('Found datastore: {ds}'.format(ds = datastore))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1615741913940
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Datastore\r\n",
        "\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can create datastores using the AzureML portal or using Python. Datastores are registered with various methods of the\r\n",
        "[`Datastore` class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore(class)?view=azure-ml-py#methods). The following\r\n",
        "example registeres a *Azure Data Lake Storage Gen2*.\r\n",
        "\r\n",
        "The goal of a separate store of datastores is that administrators can create the datastore registration. During the registration process,\r\n",
        "they need to deal with authentication and authorization. The details depend on the kind of datastore the admin wants to register\r\n",
        "(see [Connect to storage services on Azure](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-access-data) for details).\r\n",
        "\r\n",
        "The data scientists do not need to deal with auth anymore. They can use the datastores that the admins registered."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Datastore\r\n",
        "\r\n",
        "# Specify a datastore name. You will later reference the datastore with this name.\r\n",
        "ADLSGEN2_DATASTORE = 'demostore'\r\n",
        "\r\n",
        "# Get ADLS account name from Azure portal.\r\n",
        "ADLSGEN2_ACCOUNT = 'stdatasciencelab'\r\n",
        "\r\n",
        "# Get service principal from Key Vault.\r\n",
        "kv = ws.get_default_keyvault()\r\n",
        "CLIENT_ID = kv.get_secret('azureml-dls-appid')\r\n",
        "CLIENT_SECRET = kv.get_secret('azureml-dls-secret')\r\n",
        "\r\n",
        "# Unregister datastore if it already exists\r\n",
        "if ADLSGEN2_DATASTORE in ws.datastores:\r\n",
        "    print('Datastore already exists, unregistering...')\r\n",
        "    datastore = ws.datastores[ADLSGEN2_DATASTORE]\r\n",
        "    datastore.unregister()\r\n",
        "    print('Datastore unregistered')\r\n",
        "\r\n",
        "# Register datastore\r\n",
        "print('Registering datastore')\r\n",
        "datastore = Datastore.register_azure_data_lake_gen2(\r\n",
        "    workspace=ws, \r\n",
        "    datastore_name=ADLSGEN2_DATASTORE, \r\n",
        "    account_name=ADLSGEN2_ACCOUNT,\r\n",
        "    filesystem='data',\r\n",
        "    tenant_id=AAD_TENANT,\r\n",
        "    client_id=CLIENT_ID,\r\n",
        "    client_secret=CLIENT_SECRET)\r\n",
        "\r\n",
        "# Optionally, we can set the default datastore for our workspace\r\n",
        "ws.set_default_datastore(ADLSGEN2_ACCOUNT)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615722783986
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with Datasets\r\n",
        "\r\n",
        "## Registering Datasets\r\n",
        "\r\n",
        "Datasets are packaged data objects that are readily consumable. AzureML knows two types of datasets:\r\n",
        "\r\n",
        "* *File*: Unstructured data\r\n",
        "* *Tabular*: Structured data (e.g. CSV, JSON, RDBMS etc.)\r\n",
        "\r\n",
        "In this example we focus on *Tabular*. For details about methods used to create tabular datasets, see\r\n",
        "[`TabularDatasetFactory` class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.dataset_factory.tabulardatasetfactory)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Dataset, Datastore\r\n",
        "from azureml.data.datapath import DataPath\r\n",
        "\r\n",
        "# Get datastore referen ce by name or use default datastore\r\n",
        "#ds = ws.datastores[ADLSGEN2_DATASTORE]\r\n",
        "ds = ws.get_default_datastore()\r\n",
        "\r\n",
        "# Create the data path and dataset\r\n",
        "dp = DataPath(ds, 'train.jl')\r\n",
        "dset = Dataset.Tabular.from_json_lines_files(path = dp, validate = True)\r\n",
        "\r\n",
        "# Register dataset with name\r\n",
        "cooking_ds = dset.register(workspace = ws, name = \"cooking-train\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615741661718
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Datasets\r\n",
        "\r\n",
        "Once we registered the dataset, we can use it to e.g. get it as a\r\n",
        "[pandas `DataFrame`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html). Here we use the [`head`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) method\r\n",
        "to get the first few rows of our dataset."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cooking_ds = ws.datasets['cooking-train']\r\n",
        "df = cooking_ds.to_pandas_dataframe()\r\n",
        "df.head()\r\n",
        "\r\n",
        "# Have fun with pandas dataframe in `df`"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615747347188
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this sample, we have built a simple preprocessing function cleaning up our data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import preprocess\r\n",
        "print(preprocess(['Half and Half 15 ounce of Grains']))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615741952454
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can apply the preprocessing formula to our dataframe."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import preprocess\r\n",
        "X_train = df['ingredients'].apply(preprocess)\r\n",
        "Y_train = df['cuisine']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615747369602
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use a simple multinomial naive bayes model for prediction. Here we use [*scikit-learn*](https://scikit-learn.org/stable/index.html) for that."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from utils import preprocess\r\n",
        "\r\n",
        "vectorizer = CountVectorizer()\r\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\r\n",
        "\r\n",
        "mnb = MultinomialNB()\r\n",
        "mnb.fit(X_train_vec, Y_train)\r\n",
        "\r\n",
        "keywords = preprocess([\r\n",
        "        \"pork stew meat\",\r\n",
        "        \"salt\",\r\n",
        "        \"tomatoes\",\r\n",
        "        \"tomatillos\",\r\n",
        "        \"chile pepper\",\r\n",
        "        \"pepper\",\r\n",
        "        \"garlic\"\r\n",
        "        ])\r\n",
        "print(keywords)\r\n",
        "X_test = vectorizer.transform([keywords])\r\n",
        "mnb.predict(X_test)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615748526530
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can dump the model to persist it."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\r\n",
        "from pathlib import Path\r\n",
        "Path(\"./output\").mkdir(exist_ok=True)\r\n",
        "joblib.dump(mnb, './output/mnb_model.pkl')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615742272652
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Later we can reload it and do other predictions."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\r\n",
        "mnb = joblib.load('./output/mnb_model.pkl')\r\n",
        "\r\n",
        "X_test = vectorizer.transform([preprocess([\r\n",
        "      \"sugar\",\r\n",
        "      \"large egg yolks\",\r\n",
        "      \"grated lemon peel\",\r\n",
        "      \"rhubarb\",\r\n",
        "      \"cream\",\r\n",
        "      \"salt\",\r\n",
        "      \"ground cinnamon\",\r\n",
        "      \"golden brown sugar\",\r\n",
        "      \"all-purpose flour\",\r\n",
        "      \"sliced almonds\",\r\n",
        "      \"unsalted butter\"\r\n",
        "    ])])\r\n",
        "mnb.predict(X_test)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615742546240
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use the model to generate results from test data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\r\n",
        "from utils import preprocess\r\n",
        "\r\n",
        "# Read test dataframe and apply preprocessing function\r\n",
        "test_json = pd.read_json('./input/test.json')\r\n",
        "test = test_json['ingredients'].apply(preprocess)\r\n",
        "testfinal = vectorizer.transform(test)\r\n",
        "\r\n",
        "# Do prediction\r\n",
        "result = mnb.predict(testfinal)\r\n",
        "\r\n",
        "# Generate result CSV\r\n",
        "result_transformed = pd.DataFrame(result)\r\n",
        "result_with_ids = pd.concat([test_json['id'], result_transformed], join = 'outer', axis = 1)\r\n",
        "result_with_ids.to_csv('./output/result_vectorizer.csv', index = False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615744571383
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also try logical regression:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\r\n",
        "\r\n",
        "clf = LogisticRegression(max_iter=1000).fit(X_train_vec, Y_train)\r\n",
        "result = clf.predict(testfinal)\r\n",
        "\r\n",
        "result_transformed = pd.DataFrame(result)\r\n",
        "result_with_ids = pd.concat([test_json['id'], result_transformed], join = 'outer', axis = 1)\r\n",
        "result_with_ids.to_csv('./output/result_logistic_regression.csv', index = False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615744626628
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally, let's try XGB Classifier:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost\r\n",
        "from xgboost import XGBRegressor\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from numpy import loadtxt\r\n",
        "from xgboost import XGBClassifier\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "model = XGBClassifier()\r\n",
        "model.fit(X_train_vec, Y_train)\r\n",
        "result = model.predict(testfinal)\r\n",
        "\r\n",
        "result_transformed = pd.DataFrame(result)\r\n",
        "result_with_ids = pd.concat([test_json['id'], result_transformed], join = 'outer', axis = 1)\r\n",
        "result_with_ids.to_csv('./output/result_xgbclassifier.csv', index = False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615744714535
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}